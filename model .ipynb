{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1130d044",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44fb60b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-31 14:45:43.031228: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba \n",
    "import jieba.analyse\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d27289",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ddcc9",
   "metadata": {},
   "source": [
    "## Split the dataset into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f46c192f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20650x983994 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 11403990 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9f92fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d379dc9",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f47f6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "75a82767",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ce9f5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lightgbm模型参数设置，根据自己的需求调一调\n",
    "params = {\n",
    "    'task':'train',\n",
    "    'boosting_type':'gbdt',\n",
    "    'objective':'binary',\n",
    "    'metric':{'accuracy','auc','binary_logloss'},\n",
    "    'num_leaves':40,\n",
    "    'learning_rate':0.05,\n",
    "    'feature_fraction':0.9,\n",
    "    'bagging_fraction':0.8,\n",
    "    'bagging_freq':5,\n",
    "    'verbose':0,\n",
    "    'is_unbalance':True\n",
    "      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fc1bbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练参数设置\n",
    "gbm = lgb.train(params, lgb_train, num_boost_round=100, valid_sets=lgb_eval)\n",
    "#模型预测\n",
    "lgb_pre = gbm.predict(X_test) #括号中需要输入与训练时相同的数据格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f58ccee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightGBM\n",
      "Accuracy: 0.9586763518966909\n",
      "Precision: 0.9704994971505196\n",
      "Recall: 0.9451518119490695\n",
      "F1 Score: 0.9576579556731724\n"
     ]
    }
   ],
   "source": [
    "# 计算lightGBM指标\n",
    "accuracy_lightgbm = accuracy_score(y_test, lgb_pre>0.5)\n",
    "precision_lightgbm = precision_score(y_test, lgb_pre>0.5)\n",
    "recall_lightgbm = recall_score(y_test, lgb_pre>0.5)\n",
    "f1_lightgbm = f1_score(y_test, lgb_pre>0.5)\n",
    "\n",
    "print('lightGBM')\n",
    "print(f'Accuracy: {accuracy_lightgbm}')\n",
    "print(f'Precision: {precision_lightgbm}')\n",
    "print(f'Recall: {recall_lightgbm}')\n",
    "print(f'F1 Score: {f1_lightgbm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f52d574",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dfb34061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "65a71af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练xgboost模型\n",
    "# xgboost = XGBClassifier(learning_rate=0.3, n_estimators=50, max_depth=2, min_child_weight=1,\n",
    "#                           subsample=1, colsample_bytree=1, gamma=0.1, reg_alpha=0.01, reg_lambda=3)\n",
    "xgboost = XGBClassifier()\n",
    "\n",
    "xgboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "54fdea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "y_pred_xgboost = xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2054bc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgBOOST\n",
      "Accuracy: 0.9610976594027442\n",
      "Precision: 0.9712758851035405\n",
      "Recall: 0.9493960169768201\n",
      "F1 Score: 0.9602113257388146\n"
     ]
    }
   ],
   "source": [
    "# 评估\n",
    "accuracy_xbg = accuracy_score(y_test, y_pred_xgboost)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgboost)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgboost)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgboost)\n",
    "\n",
    "print('xgBOOST')\n",
    "print(f'Accuracy: {accuracy_xbg}')\n",
    "print(f'Precision: {precision_xgb}')\n",
    "print(f'Recall: {recall_xgb}')\n",
    "print(f'F1 Score: {f1_xgb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7b84f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_tweet_content label:\n",
      "XGBoost: [0]\n"
     ]
    }
   ],
   "source": [
    "# normal content\n",
    "new_tweet_content = ['这是 一条 新的 正常 推文']\n",
    "\n",
    "# depressed content\n",
    "# new_tweet_content = ['思 诺思 舒乐安定 代开 疫情 期间 开药 困难 开药 断药 关注 南京 兼职 超话志道 合盆友 解释 快 超话 传送门 南京 兼职']\n",
    "\n",
    "# feature extraction\n",
    "new_X = vectorizer.transform(new_tweet_content)\n",
    "\n",
    "# RF\n",
    "new_y_pred_rfc = xgboost.predict(new_X)\n",
    "\n",
    "print('\\nnew_tweet_content label:')\n",
    "print('XGBoost:', new_y_pred_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da5978",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cf9f7f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=13)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=13)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=13)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练随机森林模型\n",
    "rfc = RandomForestClassifier(n_estimators=13) # 修改树的棵树， 使用10折交叉验证来确定n_estimators的最佳值为13\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "776fc418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测测试集的label\n",
    "y_pred_rfc = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0396a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9063761097659403\n",
      "Precision: 0.9219430485762145\n",
      "Recall: 0.888028396256857\n",
      "F1 Score: 0.9046679815910585\n"
     ]
    }
   ],
   "source": [
    "# 计算模型评估指标\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rfc)\n",
    "precision_rf = precision_score(y_test, y_pred_rfc)\n",
    "recall_rf = recall_score(y_test, y_pred_rfc)\n",
    "f1_rf = f1_score(y_test, y_pred_rfc)\n",
    "\n",
    "print('Accuracy:', accuracy_rf)\n",
    "print('Precision:', precision_rf)\n",
    "print('Recall:', recall_rf)\n",
    "print('F1 Score:', f1_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6f9aef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_tweet_content label:\n",
      "RF: [0]\n"
     ]
    }
   ],
   "source": [
    "# normal content\n",
    "new_tweet_content = ['这是 一条 新的 正常 推文']\n",
    "\n",
    "# depressed content\n",
    "# new_tweet_content = ['思 诺思 舒乐安定 代开 疫情 期间 开药 困难 开药 断药 关注 南京 兼职 超话志道 合盆友 解释 快 超话 传送门 南京 兼职']\n",
    "\n",
    "# feature extraction\n",
    "new_X = vectorizer.transform(new_tweet_content)\n",
    "\n",
    "# RF\n",
    "new_y_pred_rfc = rfc.predict(new_X)\n",
    "\n",
    "print('\\nnew_tweet_content label:')\n",
    "print('RF:', new_y_pred_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74e10d",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11965c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练SVM模型\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "19878283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测测试集的label\n",
    "y_pred_svc = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960fd080",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04829872",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c09052a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4fb185b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Support Vector Machine:\n",
      "Accuracy: 0.947861178369653\n",
      "Precision: 0.9874520738933427\n",
      "Recall: 0.9080128205128205\n",
      "F1 Score: 0.9460677909500752\n"
     ]
    }
   ],
   "source": [
    "# 计算SVM模型评估指标\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "precision_svc = precision_score(y_test, y_pred_svc)\n",
    "recall_svc = recall_score(y_test, y_pred_svc)\n",
    "f1_svc = f1_score(y_test, y_pred_svc)\n",
    "\n",
    "print('\\nSupport Vector Machine:')\n",
    "print('Accuracy:', accuracy_svc)\n",
    "print('Precision:', precision_svc)\n",
    "print('Recall:', recall_svc)\n",
    "print('F1 Score:', f1_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ff7a4bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_tweet_content label:\n",
      "SVM: [1]\n"
     ]
    }
   ],
   "source": [
    "# normal content\n",
    "# new_tweet_content = ['这是 一条 新的 正常 推文']\n",
    "\n",
    "# depressed content\n",
    "new_tweet_content = ['思 诺思 舒乐安定 代开 疫情 期间 开药 困难 开药 断药 关注 南京 兼职 超话志道 合盆友 解释 快 超话 传送门 南京 兼职']\n",
    "\n",
    "# feature extraction\n",
    "new_X = vectorizer.transform(new_tweet_content)\n",
    "\n",
    "# SVM\n",
    "new_y_pred_svc = svc.predict(new_X)\n",
    "\n",
    "print('\\nnew_tweet_content label:')\n",
    "print('SVM:', new_y_pred_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f350a3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_tweet_content label:\n",
      "SVM: [1]\n",
      "RF: [1]\n"
     ]
    }
   ],
   "source": [
    "# normal content\n",
    "# new_tweet_content = ['这是 一条 新的 正常 推文']\n",
    "\n",
    "# depressed content\n",
    "new_tweet_content = ['思 诺思 舒乐安定 代开 疫情 期间 开药 困难 开药 断药 关注 南京 兼职 超话志道 合盆友 解释 快 超话 传送门 南京 兼职']\n",
    "\n",
    "# feature extraction\n",
    "new_X = vectorizer.transform(new_tweet_content)\n",
    "\n",
    "# SVM\n",
    "new_y_pred_svc = svc.predict(new_X)\n",
    "new_y_pred_rfc = rfc.predict(new_X)\n",
    "\n",
    "print('\\nnew_tweet_content label:')\n",
    "print('SVM:', new_y_pred_svc)\n",
    "print('RF:', new_y_pred_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a0428",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d717dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.387 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "try:\n",
    "    balanced_df_all_cleaned_tokenization = pd.read_csv('~/bert/balanced_df_all_cleaned_tokenization.csv')\n",
    "except:\n",
    "    balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned_tokenization.csv')\n",
    "balanced_df_all_cleaned_tokenization['tweet_content'] = balanced_df_all_cleaned_tokenization['tweet_content'].fillna('')\n",
    "df3 = balanced_df_all_cleaned_tokenization.copy()\n",
    "\n",
    "# Preprocess the data\n",
    "X = df3['tweet_content']\n",
    "y = df3['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Tokenize the sequences using Jieba tokenizer\n",
    "def tokenize(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "X_train_sequences = [' '.join(tokenize(x)) for x in X_train]\n",
    "X_test_sequences = [' '.join(tokenize(x)) for x in X_test]\n",
    "\n",
    "\n",
    "# Convert tokens to integer sequences\n",
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_sequences)\n",
    "\n",
    "# Convert tokenized sequences to integer sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_sequences)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51ad075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_length = 700  # Set the maximum sequence length for padding\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abba5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequence, label):\n",
    "        self.sequence = sequence\n",
    "        self.label = label.values\n",
    "        self.attention_mask = np.where(sequence==0, 0, 1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequence[index], self.attention_mask[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence)\n",
    "    \n",
    "\n",
    "train_dataset = TextDataset(X_train_padded, y_train)\n",
    "test_dataset = TextDataset(X_test_padded, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634cc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForSequenceClassification, DistilBertConfig\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "052fa964",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size=len(tokenizer.word_index)+1, hidden_size=32, num_hidden_layers=1, num_attention_heads=2, intermediate_size=32, max_sequence_length=max_sequence_length, max_position_embeddings=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "777005ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c888c7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 0.6930980086326599\n",
      "Epoch: 0, Step: 10, Loss: 0.6923404932022095\n",
      "Epoch: 0, Step: 20, Loss: 0.6892273426055908\n",
      "Epoch: 0, Step: 30, Loss: 0.6808115839958191\n",
      "Epoch: 0, Step: 40, Loss: 0.598413348197937\n",
      "Epoch: 0, Step: 50, Loss: 0.3680679500102997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:02<00:02,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step: 0, Loss: 0.19603076577186584\n",
      "Epoch: 1, Step: 10, Loss: 0.196017786860466\n",
      "Epoch: 1, Step: 20, Loss: 0.16065713763237\n",
      "Epoch: 1, Step: 30, Loss: 0.190627321600914\n",
      "Epoch: 1, Step: 40, Loss: 0.15493103861808777\n",
      "Epoch: 1, Step: 50, Loss: 0.10660117119550705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.99s/it]\n"
     ]
    }
   ],
   "source": [
    "bert = BertForSequenceClassification(config).to(device)\n",
    "\n",
    "bert.train()\n",
    "optimizer = AdamW(bert.parameters())\n",
    "\n",
    "epochs = 2\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        sequence, attention_mask, label = batch\n",
    "        outputs = bert(sequence, attention_mask=attention_mask, labels=label)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Step: {i}, Loss: {loss.cpu().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4614b003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 98.61it/s]\n"
     ]
    }
   ],
   "source": [
    "bert.eval()\n",
    "predictions, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_dataloader):\n",
    "        data = [item.to(device) for item in data]\n",
    "        outputs = bert(sequence, attention_mask=attention_mask, labels=label)\n",
    "        predicted_label = F.sigmoid(outputs.logits).argmax(axis=1)\n",
    "        predictions.append(predicted_label.cpu())\n",
    "        labels.append(label.cpu())\n",
    "\n",
    "\n",
    "predictions = torch.cat(predictions).numpy()\n",
    "labels = torch.cat(labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d42b9e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT:\n",
      "Accuracy: 0.9747899159663865\n",
      "Precision: 0.9710144927536232\n",
      "Recall: 0.9852941176470589\n",
      "F1 Score: 0.9781021897810219\n"
     ]
    }
   ],
   "source": [
    "# 计算BERT模型评估指标\n",
    "accuracy_bert = accuracy_score(labels, predictions)\n",
    "precision_bert = precision_score(labels, predictions)\n",
    "recall_bert = recall_score(labels, predictions)\n",
    "f1_bert = f1_score(labels, predictions)\n",
    "\n",
    "print('\\nBERT:')\n",
    "print('Accuracy:', accuracy_bert)\n",
    "print('Precision:', precision_bert)\n",
    "print('Recall:', recall_bert)\n",
    "print('F1 Score:', f1_bert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51634eeb",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be866bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cae1046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(800193, 32, padding_idx=1)\n",
       "      (position_embeddings): Embedding(1400, 32, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 32)\n",
       "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_config = RobertaConfig(vocab_size=len(tokenizer.word_index)+1, hidden_size=32, num_hidden_layers=1, num_attention_heads=2, intermediate_size=32, max_position_embeddings=max_sequence_length * 2)\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "roberta = RobertaForSequenceClassification(roberta_config)\n",
    "roberta.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "402e913d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 0.6928017735481262\n",
      "Epoch: 0, Step: 10, Loss: 0.692473828792572\n",
      "Epoch: 0, Step: 20, Loss: 0.6899939179420471\n",
      "Epoch: 0, Step: 30, Loss: 0.6554629802703857\n",
      "Epoch: 0, Step: 40, Loss: 0.623778223991394\n",
      "Epoch: 0, Step: 50, Loss: 0.4388459026813507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [01:23<01:23, 83.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step: 0, Loss: 0.1890697032213211\n",
      "Epoch: 1, Step: 10, Loss: 0.163442462682724\n",
      "Epoch: 1, Step: 20, Loss: 0.20919883251190186\n",
      "Epoch: 1, Step: 30, Loss: 0.13392075896263123\n",
      "Epoch: 1, Step: 40, Loss: 0.1986142098903656\n",
      "Epoch: 1, Step: 50, Loss: 0.11977552622556686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:49<00:00, 84.56s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(roberta.parameters())\n",
    "\n",
    "epochs = 2\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        sequence, attention_mask, label = batch\n",
    "        outputs = roberta(sequence, attention_mask=attention_mask, labels=label)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Step: {i}, Loss: {loss.cpu().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "031cf352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "roberta.eval()\n",
    "predictions, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_dataloader):\n",
    "        data = [item.to(device) for item in data]\n",
    "        outputs = roberta(sequence, attention_mask=attention_mask, labels=label)\n",
    "        predicted_label = F.sigmoid(outputs.logits).argmax(axis=1)\n",
    "        predictions.append(predicted_label.cpu())\n",
    "        labels.append(label.cpu())\n",
    "\n",
    "\n",
    "predictions = torch.cat(predictions).numpy()\n",
    "labels = torch.cat(labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6db43950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta:\n",
      "Accuracy: 0.9747899159663865\n",
      "Precision: 0.9827586206896551\n",
      "Recall: 0.9661016949152542\n",
      "F1 Score: 0.9743589743589743\n"
     ]
    }
   ],
   "source": [
    "# 计算Roberta模型评估指标\n",
    "accuracy_bert = accuracy_score(labels, predictions)\n",
    "precision_bert = precision_score(labels, predictions)\n",
    "recall_bert = recall_score(labels, predictions)\n",
    "f1_bert = f1_score(labels, predictions)\n",
    "\n",
    "print('Roberta:')\n",
    "print('Accuracy:', accuracy_bert)\n",
    "print('Precision:', precision_bert)\n",
    "print('Recall:', recall_bert)\n",
    "print('F1 Score:', f1_bert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7717fc4",
   "metadata": {},
   "source": [
    "## TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ff55506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "408e666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned_tokenization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6cbead3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df_all_cleaned_tokenization['tweet_content'] = balanced_df_all_cleaned_tokenization['tweet_content'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7177af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = balanced_df_all_cleaned_tokenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12c8bc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15315</td>\n",
       "      <td>0</td>\n",
       "      <td>害真 挺 吃 颜想 岁 足 表达 减肥 决心 姐妹 谈恋爱 什 感觉 全世界 闺蜜 介绍 成...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17978</td>\n",
       "      <td>0</td>\n",
       "      <td>拥抱 世界 暖心 动作 生活 中 难免 遇 挫折 坎坷 安 慌乱 许时 拥抱 会 充满 量 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>雨天 穿会 滑倒 坐水里 亲测 艾特子 赟 相信 家会 意见 麻雀 妈妈 问 麻雀 天扎什 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16604</td>\n",
       "      <td>0</td>\n",
       "      <td>种 幸福 做 桌菜 然家 朋友 满足 吃 完 没 帮忙 刷碗 老规矩 昨晚 想 做 早餐 奶...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20755</td>\n",
       "      <td>0</td>\n",
       "      <td>日 清明 青草 疫情 中 牺牲 医护 员 公安干警 基层干部 线 工作 逝世 胞 表示 沉痛...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  label                                      tweet_content\n",
       "0  15315      0  害真 挺 吃 颜想 岁 足 表达 减肥 决心 姐妹 谈恋爱 什 感觉 全世界 闺蜜 介绍 成...\n",
       "1  17978      0  拥抱 世界 暖心 动作 生活 中 难免 遇 挫折 坎坷 安 慌乱 许时 拥抱 会 充满 量 ...\n",
       "2    221      0  雨天 穿会 滑倒 坐水里 亲测 艾特子 赟 相信 家会 意见 麻雀 妈妈 问 麻雀 天扎什 ...\n",
       "3  16604      0  种 幸福 做 桌菜 然家 朋友 满足 吃 完 没 帮忙 刷碗 老规矩 昨晚 想 做 早餐 奶...\n",
       "4  20755      0  日 清明 青草 疫情 中 牺牲 医护 员 公安干警 基层干部 线 工作 逝世 胞 表示 沉痛..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4dd5c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X = df3['tweet_content']\n",
    "y = df3['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21976f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfced35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sequences using Jieba tokenizer\n",
    "def tokenize(text):\n",
    "    return list(jieba.cut(text))\n",
    "X_train_sequences = [' '.join(tokenize(x)) for x in X_train]\n",
    "X_test_sequences = [' '.join(tokenize(x)) for x in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "23483787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to integer sequences\n",
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_sequences)\n",
    "\n",
    "# Convert tokenized sequences to integer sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_sequences)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19999162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "max_sequence_length = 7000  # Set the maximum sequence length for padding\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d5cc6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6195, 7000)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_padded.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
